{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:40:36.297616: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/rsoedarnadi/Documents/GitHub/Datathon-Dataverse/Excel Files/Traffic_Data_Department_Total.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Department\"] = label_encoder.fit_transform(df[\"Department\"])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "df[[\"Accident_Count\"]] = scaler.fit_transform(df[[\"Accident_Count\"]])\n",
    "\n",
    "# Sort by department and year\n",
    "df = df.sort_values(by=[\"Department\", \"Year\"]).reset_index(drop=True)\n",
    "\n",
    "# Define sequence length\n",
    "SEQ_LENGTH = 10\n",
    "X, y = [], []\n",
    "\n",
    "departments = df[\"Department\"].unique()\n",
    "for dept in departments:\n",
    "    dept_data = df[df[\"Department\"] == dept].reset_index(drop=True)\n",
    "    for i in range(len(dept_data) - SEQ_LENGTH):\n",
    "        X.append(dept_data.iloc[i:i+SEQ_LENGTH][[\"Accident_Count\"]].values)\n",
    "        y.append(dept_data.iloc[i+SEQ_LENGTH][\"Accident_Count\"])\n",
    "\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "3/3 [==============================] - 8s 457ms/step - loss: 0.6303 - val_loss: 0.4910\n",
      "Epoch 2/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.3603 - val_loss: 0.5986\n",
      "Epoch 3/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5123 - val_loss: 0.6300\n",
      "Epoch 4/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5207 - val_loss: 0.5951\n",
      "Epoch 5/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4862 - val_loss: 0.5262\n",
      "Epoch 6/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4306 - val_loss: 0.4007\n",
      "Epoch 7/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.3240 - val_loss: 0.1278\n",
      "Epoch 8/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1634 - val_loss: 0.2640\n",
      "Epoch 9/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.1392 - val_loss: 0.1937\n",
      "Epoch 10/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.2359 - val_loss: 0.0352\n",
      "Epoch 11/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1546 - val_loss: 0.1328\n",
      "Epoch 12/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.1815 - val_loss: 0.0538\n",
      "Epoch 13/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.1555 - val_loss: 0.1041\n",
      "Epoch 14/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.1674 - val_loss: 0.0606\n",
      "Epoch 15/60\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.1330 - val_loss: 0.0751\n",
      "Epoch 16/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.1320 - val_loss: 0.0520\n",
      "Epoch 17/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1017 - val_loss: 0.0790\n",
      "Epoch 18/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0997 - val_loss: 0.1276\n",
      "Epoch 19/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.1071 - val_loss: 0.1015\n",
      "Epoch 20/60\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.1098 - val_loss: 0.0635\n",
      "Epoch 21/60\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0977 - val_loss: 0.0435\n",
      "Epoch 22/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0839 - val_loss: 0.0772\n",
      "Epoch 23/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0760 - val_loss: 0.0541\n",
      "Epoch 24/60\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0797 - val_loss: 0.0299\n",
      "Epoch 25/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0770 - val_loss: 0.0584\n",
      "Epoch 26/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0859 - val_loss: 0.0797\n",
      "Epoch 27/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0667 - val_loss: 0.0447\n",
      "Epoch 28/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0871 - val_loss: 0.0885\n",
      "Epoch 29/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0887 - val_loss: 0.0770\n",
      "Epoch 30/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0985 - val_loss: 0.0882\n",
      "Epoch 31/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0816 - val_loss: 0.0564\n",
      "Epoch 32/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0830 - val_loss: 0.0537\n",
      "Epoch 33/60\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0957 - val_loss: 0.0530\n",
      "Epoch 34/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0615 - val_loss: 0.0806\n",
      "Epoch 35/60\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0908 - val_loss: 0.0934\n",
      "Epoch 36/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0794 - val_loss: 0.0294\n",
      "Epoch 37/60\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0745 - val_loss: 0.0750\n",
      "Epoch 38/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0834 - val_loss: 0.0384\n",
      "Epoch 39/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0710 - val_loss: 0.0742\n",
      "Epoch 40/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0746 - val_loss: 0.0605\n",
      "Epoch 41/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0862 - val_loss: 0.0639\n",
      "Epoch 42/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0675 - val_loss: 0.0348\n",
      "Epoch 43/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0799 - val_loss: 0.0289\n",
      "Epoch 44/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0729 - val_loss: 0.0631\n",
      "Epoch 45/60\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0741 - val_loss: 0.0877\n",
      "Epoch 46/60\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0869 - val_loss: 0.0290\n",
      "Epoch 47/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0681 - val_loss: 0.0647\n",
      "Epoch 48/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0737 - val_loss: 0.0617\n",
      "Epoch 49/60\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0653 - val_loss: 0.0793\n",
      "Epoch 50/60\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0752 - val_loss: 0.1075\n",
      "Epoch 51/60\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0749 - val_loss: 0.0406\n",
      "Epoch 52/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0781 - val_loss: 0.0362\n",
      "Epoch 53/60\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0684 - val_loss: 0.0429\n",
      "Epoch 54/60\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0648 - val_loss: 0.0300\n",
      "Epoch 55/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0758 - val_loss: 0.0669\n",
      "Epoch 56/60\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0732 - val_loss: 0.0832\n",
      "Epoch 57/60\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0821 - val_loss: 0.0610\n",
      "Epoch 58/60\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0735 - val_loss: 0.0348\n",
      "Epoch 59/60\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0698 - val_loss: 0.0364\n",
      "Epoch 60/60\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0669 - val_loss: 0.0301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e3e01cb20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "model = Sequential([Bidirectional(LSTM(100, activation='relu', return_sequences=True), input_shape=(SEQ_LENGTH, 1)),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(100, activation='relu')),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='mae')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=60, batch_size=16, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.32766825],\n",
       "        [0.89734874],\n",
       "        [0.91298436],\n",
       "        [0.90890551],\n",
       "        [0.7675051 ],\n",
       "        [0.88987084],\n",
       "        [0.92318151],\n",
       "        [0.47382733],\n",
       "        [0.64649898],\n",
       "        [0.92318151]],\n",
       "\n",
       "       [[0.89734874],\n",
       "        [0.91298436],\n",
       "        [0.90890551],\n",
       "        [0.7675051 ],\n",
       "        [0.88987084],\n",
       "        [0.92318151],\n",
       "        [0.47382733],\n",
       "        [0.64649898],\n",
       "        [0.92318151],\n",
       "        [0.73555404]],\n",
       "\n",
       "       [[0.91298436],\n",
       "        [0.90890551],\n",
       "        [0.7675051 ],\n",
       "        [0.88987084],\n",
       "        [0.92318151],\n",
       "        [0.47382733],\n",
       "        [0.64649898],\n",
       "        [0.92318151],\n",
       "        [0.73555404],\n",
       "        [0.760707  ]],\n",
       "\n",
       "       [[0.90890551],\n",
       "        [0.7675051 ],\n",
       "        [0.88987084],\n",
       "        [0.92318151],\n",
       "        [0.47382733],\n",
       "        [0.64649898],\n",
       "        [0.92318151],\n",
       "        [0.73555404],\n",
       "        [0.760707  ],\n",
       "        [0.77022434]],\n",
       "\n",
       "       [[0.4085656 ],\n",
       "        [0.36165874],\n",
       "        [0.22229776],\n",
       "        [0.59755269],\n",
       "        [0.73351462],\n",
       "        [0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ]],\n",
       "\n",
       "       [[0.36165874],\n",
       "        [0.22229776],\n",
       "        [0.59755269],\n",
       "        [0.73351462],\n",
       "        [0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ],\n",
       "        [0.57239973]],\n",
       "\n",
       "       [[0.22229776],\n",
       "        [0.59755269],\n",
       "        [0.73351462],\n",
       "        [0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ],\n",
       "        [0.57239973],\n",
       "        [0.7185588 ]],\n",
       "\n",
       "       [[0.59755269],\n",
       "        [0.73351462],\n",
       "        [0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ],\n",
       "        [0.57239973],\n",
       "        [0.7185588 ],\n",
       "        [0.62610469]],\n",
       "\n",
       "       [[0.73351462],\n",
       "        [0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ],\n",
       "        [0.57239973],\n",
       "        [0.7185588 ],\n",
       "        [0.62610469],\n",
       "        [0.72807614]],\n",
       "\n",
       "       [[0.82936778],\n",
       "        [0.84568321],\n",
       "        [0.7185588 ],\n",
       "        [0.81305235],\n",
       "        [0.492862  ],\n",
       "        [0.57239973],\n",
       "        [0.7185588 ],\n",
       "        [0.62610469],\n",
       "        [0.72807614],\n",
       "        [0.74915024]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 657ms/step\n",
      "MAPE: 0.03745787058961402\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "y_pred_original = scaler.inverse_transform(y_pred).flatten()\n",
    "\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test_original,y_pred_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Department Al Maamora:\n",
      "  Year 2024: Predicted Accidents 1293.90\n",
      "  Year 2025: Predicted Accidents 1289.85\n",
      "  Year 2026: Predicted Accidents 1285.83\n",
      "Department Al Rayyan:\n",
      "  Year 2024: Predicted Accidents 1280.42\n",
      "  Year 2025: Predicted Accidents 1280.20\n",
      "  Year 2026: Predicted Accidents 1279.26\n",
      "Department Al Shammal:\n",
      "  Year 2024: Predicted Accidents 1272.50\n",
      "  Year 2025: Predicted Accidents 1270.66\n",
      "  Year 2026: Predicted Accidents 1269.12\n",
      "Department Al Thumama (Al Mattar):\n",
      "  Year 2024: Predicted Accidents 1251.67\n",
      "  Year 2025: Predicted Accidents 1258.48\n",
      "  Year 2026: Predicted Accidents 1262.06\n",
      "Department Dukhan:\n",
      "  Year 2024: Predicted Accidents 187.80\n",
      "  Year 2025: Predicted Accidents 180.49\n",
      "  Year 2026: Predicted Accidents 174.98\n",
      "Department Industerid area:\n",
      "  Year 2024: Predicted Accidents 883.20\n",
      "  Year 2025: Predicted Accidents 879.53\n",
      "  Year 2026: Predicted Accidents 880.27\n",
      "Department Madinatt khalifah:\n",
      "  Year 2024: Predicted Accidents 1279.96\n",
      "  Year 2025: Predicted Accidents 1279.40\n",
      "  Year 2026: Predicted Accidents 1276.42\n",
      "Department South :\n",
      "  Year 2024: Predicted Accidents 1263.90\n",
      "  Year 2025: Predicted Accidents 1262.74\n",
      "  Year 2026: Predicted Accidents 1263.46\n"
     ]
    }
   ],
   "source": [
    "# Forecast for 2024-2026\n",
    "future_years = [2024, 2025, 2026]\n",
    "forecast_results = {}\n",
    "\n",
    "for dept in departments:\n",
    "    dept_data = df[df[\"Department\"] == dept].reset_index(drop=True)\n",
    "    last_sequence = dept_data.iloc[-SEQ_LENGTH:][[\"Accident_Count\"]].values.reshape(1, SEQ_LENGTH, 1)\n",
    "    predictions = []\n",
    "    \n",
    "    for year in future_years:\n",
    "        pred = model.predict(last_sequence)[0, 0]\n",
    "        predictions.append(pred)\n",
    "        last_sequence = np.roll(last_sequence, -1)\n",
    "        last_sequence[0, -1, 0] = pred\n",
    "    \n",
    "    # Convert predictions back to original scale\n",
    "    predictions_original = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    forecast_results[dept] = dict(zip(future_years, predictions_original))\n",
    "\n",
    "# Print future predictions\n",
    "for dept, predictions in forecast_results.items():\n",
    "    # Transform the encoded department value back to the original name\n",
    "    original_dept = label_encoder.inverse_transform([dept])[0]\n",
    "    print(f\"Department {original_dept}:\")\n",
    "    for year, pred in predictions.items():\n",
    "        print(f\"  Year {year}: Predicted Accidents {pred:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Department  Year  Accident Count\n",
      "0               Al Maamora  2024     1287.551758\n",
      "1        Madinatt khalifah  2024     1272.090088\n",
      "2                   South   2024     1254.616333\n",
      "3                Al Rayyan  2024     1272.573608\n",
      "4                   Dukhan  2024      241.565079\n",
      "5               Al Shammal  2024     1264.872681\n",
      "6          Industerid area  2024      672.217712\n",
      "7   Al Thumama (Al Mattar)  2024     1237.489136\n",
      "8                   Dukhan  2025      236.500839\n",
      "9                   South   2025     1250.442383\n",
      "10       Madinatt khalifah  2025     1270.533569\n",
      "11              Al Shammal  2025     1261.274536\n",
      "12               Al Rayyan  2025     1270.730469\n",
      "13              Al Maamora  2025     1281.941895\n",
      "14  Al Thumama (Al Mattar)  2025     1240.850342\n",
      "15         Industerid area  2025      560.402100\n",
      "16       Madinatt khalifah  2026     1265.881348\n",
      "17  Al Thumama (Al Mattar)  2026     1244.165283\n",
      "18                  Dukhan  2026      233.462418\n",
      "19              Al Shammal  2026     1257.543823\n",
      "20               Al Rayyan  2026     1268.671631\n",
      "21              Al Maamora  2026     1276.315186\n",
      "22         Industerid area  2026      457.434814\n",
      "23                  South   2026     1250.861694\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the results\n",
    "results = []\n",
    "# Loop through forecast results and compile the data\n",
    "for dept, predictions in forecast_results.items():\n",
    "    original_dept = label_encoder.inverse_transform([dept])[0]\n",
    "    for year, pred in predictions.items():\n",
    "        results.append({\n",
    "            \"Department\": original_dept,\n",
    "            \"Year\": year,\n",
    "            \"Accident Count\": pred\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "df_results = df_results.sort_values(by=\"Year\")\n",
    "\n",
    "# Reset the index\n",
    "df_results = df_results.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"./Traffic Dept Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/rsoedarnadi/Downloads/traffic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rsoedarnadi/Desktop/DataVerse/encoder.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,\"/Users/rsoedarnadi/Desktop/DataVerse/scaler.pkl\")\n",
    "joblib.dump(label_encoder,\"/Users/rsoedarnadi/Desktop/DataVerse/encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_22 (Bidirecti  (None, 10, 200)          81600     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 10, 200)           0         \n",
      "                                                                 \n",
      " bidirectional_23 (Bidirecti  (None, 200)              240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,601\n",
      "Trainable params: 322,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model = load_model('/Users/rsoedarnadi/Desktop/DataVerse/traffic.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved pipeline components\n",
    "new_label_encoder = joblib.load(\"/Users/rsoedarnadi/Desktop/DataVerse/encoder.pkl\")\n",
    "new_scaler = joblib.load(\"/Users/rsoedarnadi/Desktop/DataVerse/scaler.pkl\")\n",
    "\n",
    "# Define sequence length\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# Load dataset for reference\n",
    "file_path = \"/Users/rsoedarnadi/Desktop/DataVerse/Traffic_Data_Department_Total.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure department names are consistent\n",
    "df[\"Department\"] = df[\"Department\"].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_accidents(department, year_to_forecast):\n",
    "    # Verify department exists\n",
    "    if department not in df[\"Department\"].unique():\n",
    "        raise ValueError(f\"Department '{department}' not found. Available departments: {df['Department'].unique()}\")\n",
    "    \n",
    "    encoded_dept = new_label_encoder.transform([department])[0]\n",
    "    dept_data = df[df[\"Department\"] == department].reset_index(drop=True)\n",
    "    \n",
    "    if len(dept_data) < SEQ_LENGTH:\n",
    "        raise ValueError(\"Not enough data for this department to make a prediction\")\n",
    "    \n",
    "    # Get last SEQ_LENGTH accident counts and normalize\n",
    "    last_sequence = dept_data.iloc[-SEQ_LENGTH:][[\"Accident_Count\"]]\n",
    "    last_sequence = pd.DataFrame(last_sequence, columns=[\"Accident_Count\"])\n",
    "    last_sequence = new_scaler.transform(last_sequence).reshape(1, SEQ_LENGTH, 1)\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(year_to_forecast - dept_data[\"Year\"].max()):\n",
    "        pred = new_model.predict(last_sequence)[0, 0]\n",
    "        predictions.append(pred)\n",
    "        last_sequence = np.roll(last_sequence, -1)\n",
    "        last_sequence[0, -1, 0] = pred\n",
    "    \n",
    "    # Convert predictions back to original scale\n",
    "    predictions_original = new_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return predictions_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "department = \"Dukhan\"  # Change this to an actual department from your dataset\n",
    "year_to_forecast = 2026\n",
    "\n",
    "try:\n",
    "    predictions = predict_accidents(department, year_to_forecast)\n",
    "    print(f\"Predicted accident counts for {department} in {year_to_forecast}: {predictions[-1]:.2f}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
