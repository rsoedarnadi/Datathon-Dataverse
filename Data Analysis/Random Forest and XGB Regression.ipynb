{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/rsoedarnadi/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/rsoedarnadi/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.8.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Traffic_Data_Nationality.xlsx\"  # Update with the correct path if needed\n",
    "xls = pd.ExcelFile(file_path)\n",
    "# Load the first sheet\n",
    "df = pd.read_excel(xls, sheet_name=\"Nationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define target and predictor variables\n",
    "target = 'Injury_Severity'\n",
    "predictors = ['Count', 'Gender', 'Nationality', 'Role', 'Year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Injury_Severity</th>\n",
       "      <th>Nationality_Foreign</th>\n",
       "      <th>Nationality_G.C.C</th>\n",
       "      <th>Nationality_Not Stated</th>\n",
       "      <th>Nationality_Other Arabs</th>\n",
       "      <th>Nationality_Qataris</th>\n",
       "      <th>Role_Driver</th>\n",
       "      <th>Role_Passenger</th>\n",
       "      <th>Role_Pedestrian</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Year_Scaled</th>\n",
       "      <th>Count_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.263175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.313336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.286780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.145149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.316286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Injury_Severity  Nationality_Foreign  Nationality_G.C.C  \\\n",
       "0                1                    0                  0   \n",
       "1                1                    0                  1   \n",
       "2                1                    0                  0   \n",
       "3                1                    1                  0   \n",
       "4                1                    0                  0   \n",
       "\n",
       "   Nationality_Not Stated  Nationality_Other Arabs  Nationality_Qataris  \\\n",
       "0                       0                        0                    1   \n",
       "1                       0                        0                    0   \n",
       "2                       0                        1                    0   \n",
       "3                       0                        0                    0   \n",
       "4                       1                        0                    0   \n",
       "\n",
       "   Role_Driver  Role_Passenger  Role_Pedestrian  Gender_Female  Gender_Male  \\\n",
       "0            1               0                0              0            1   \n",
       "1            1               0                0              0            1   \n",
       "2            1               0                0              0            1   \n",
       "3            1               0                0              0            1   \n",
       "4            1               0                0              0            1   \n",
       "\n",
       "   Year_Scaled  Count_Scaled  \n",
       "0     1.512243     -0.263175  \n",
       "1     1.512243     -0.313336  \n",
       "2     1.512243     -0.286780  \n",
       "3     1.512243     -0.145149  \n",
       "4     1.512243     -0.316286  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "#encode categorical predictor variables (One-Hot Encoding)\n",
    "df = pd.get_dummies(df,columns=['Nationality','Role','Gender'])\n",
    "\n",
    "# Standardize numerical feature\n",
    "scaler = StandardScaler()\n",
    "df['Year_Scaled'] = scaler.fit_transform(df[['Year']])\n",
    "df['Count_Scaled'] = scaler.fit_transform(df[['Count']])\n",
    "Severity = {'Deaths': 1, 'SevereInjury':1, 'SlightInjury':0}\n",
    "df['Injury_Severity'] = df['Injury_Severity'].map(Severity)\n",
    "\n",
    "# Concatenate encoded categorical features and scaled numerical feature\n",
    "final_data = df.drop(['Count','Year'], axis=1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Injury_Severity</th>\n",
       "      <th>Nationality_Foreign</th>\n",
       "      <th>Nationality_G.C.C</th>\n",
       "      <th>Nationality_Not Stated</th>\n",
       "      <th>Nationality_Other Arabs</th>\n",
       "      <th>Role_Passenger</th>\n",
       "      <th>Role_Pedestrian</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Year_Scaled</th>\n",
       "      <th>Count_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.263175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.313336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.286780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.145149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.512243</td>\n",
       "      <td>-0.316286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Injury_Severity  Nationality_Foreign  Nationality_G.C.C  \\\n",
       "0                1                    0                  0   \n",
       "1                1                    0                  1   \n",
       "2                1                    0                  0   \n",
       "3                1                    1                  0   \n",
       "4                1                    0                  0   \n",
       "\n",
       "   Nationality_Not Stated  Nationality_Other Arabs  Role_Passenger  \\\n",
       "0                       0                        0               0   \n",
       "1                       0                        0               0   \n",
       "2                       0                        1               0   \n",
       "3                       0                        0               0   \n",
       "4                       1                        0               0   \n",
       "\n",
       "   Role_Pedestrian  Gender_Male  Year_Scaled  Count_Scaled  \n",
       "0                0            1     1.512243     -0.263175  \n",
       "1                0            1     1.512243     -0.313336  \n",
       "2                0            1     1.512243     -0.286780  \n",
       "3                0            1     1.512243     -0.145149  \n",
       "4                0            1     1.512243     -0.316286  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop one category from each dummy variable to avoid multicollinearity\n",
    "final_data = final_data.drop(['Nationality_Qataris','Role_Driver','Gender_Female'], axis=1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Results:\n",
      "                   Feature       VIF\n",
      "0      Nationality_Foreign  1.482426\n",
      "1        Nationality_G.C.C  1.417787\n",
      "2   Nationality_Not Stated  1.302468\n",
      "3  Nationality_Other Arabs  1.402234\n",
      "4           Role_Passenger  1.626375\n",
      "5          Role_Pedestrian  1.703971\n",
      "6              Gender_Male  1.826278\n",
      "7              Year_Scaled  1.078041\n",
      "8             Count_Scaled  1.197734\n"
     ]
    }
   ],
   "source": [
    "# Check for multicollinearity (VIF)\n",
    "def calculate_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = df.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "vif_results = calculate_vif(final_data.drop(columns=[target]))\n",
    "print(\"VIF Results:\")\n",
    "print(vif_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Polynomial features for interaction terms\\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\\nX_train_poly = poly.fit_transform(X_train)\\nX_test_poly = poly.transform(X_test) '"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split\n",
    "X = final_data.drop(columns=[target])\n",
    "y = final_data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\"\"\" # Polynomial features for interaction terms\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(eval_metric='mlogloss')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.59      0.62        71\n",
      "           1       0.80      0.84      0.82       138\n",
      "\n",
      "    accuracy                           0.76       209\n",
      "   macro avg       0.73      0.72      0.72       209\n",
      "weighted avg       0.75      0.76      0.75       209\n",
      "\n",
      "Random Forest Accuracy: 0.9234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88        71\n",
      "           1       0.93      0.96      0.94       138\n",
      "\n",
      "    accuracy                           0.92       209\n",
      "   macro avg       0.92      0.91      0.91       209\n",
      "weighted avg       0.92      0.92      0.92       209\n",
      "\n",
      "XGBoost Accuracy: 0.9282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90        71\n",
      "           1       0.95      0.94      0.95       138\n",
      "\n",
      "    accuracy                           0.93       209\n",
      "   macro avg       0.92      0.92      0.92       209\n",
      "weighted avg       0.93      0.93      0.93       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression cross-validation scores: [0.84210526 0.77033493 0.80382775 0.83732057 0.83173077]\n",
      "Logistic regression mean accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that combines polynomial feature generation and logistic regression\n",
    "logreg_pipeline = make_pipeline(\n",
    "    #PolynomialFeatures(degree=2,interaction_only=True, include_bias=False),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "logreg_cv_scores = cross_val_score(logreg_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(f'Logistic regression cross-validation scores: {logreg_cv_scores}')\n",
    "print(f'Logistic regression mean accuracy: {logreg_cv_scores.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest cross-validation scores: [0.96172249 0.92344498 0.91866029 0.97607656 0.97115385]\n",
      "Random forest mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that combines polynomial feature generation and logistic regression\n",
    "rf_pipeline = make_pipeline(\n",
    "    #PolynomialFeatures(degree=2,interaction_only=True, include_bias=False),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "rf_cv_scores = cross_val_score(rf_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(f'Random forest cross-validation scores: {rf_cv_scores}')\n",
    "print(f'Random forest mean accuracy: {rf_cv_scores.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost cross-validation scores: [0.97607656 0.93779904 0.94736842 0.97607656 0.95673077]\n",
      "XGBoost regression mean accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that combines polynomial feature generation and logistic regression\n",
    "xgb_pipeline = make_pipeline(\n",
    "    #PolynomialFeatures(degree=2,interaction_only=True, include_bias=False),\n",
    "    XGBClassifier(eval_metric='mlogloss')\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "xgb_cv_scores = cross_val_score(xgb_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores and mean accuracy\n",
    "print(f'XGBoost cross-validation scores: {xgb_cv_scores}')\n",
    "print(f'XGBoost regression mean accuracy: {xgb_cv_scores.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Coefficients and 95% Confidence Intervals:\n",
      "                         Coefficient  Standard Error  95% CI Lower Bound  \\\n",
      "Nationality_Foreign         1.196366        0.104064            0.992405   \n",
      "Nationality_G.C.C          -1.189671        0.105140           -1.395741   \n",
      "Nationality_Not Stated     -1.490987        0.129099           -1.744016   \n",
      "Nationality_Other Arabs     0.442823        0.103012            0.240924   \n",
      "Role_Passenger              0.116309        0.085570           -0.051405   \n",
      "Role_Pedestrian            -1.041886        0.086939           -1.212282   \n",
      "Gender_Male                 1.055365        0.070847            0.916507   \n",
      "Year_Scaled                 0.089271        0.036166            0.018387   \n",
      "Count_Scaled               -7.461648        0.035319           -7.530871   \n",
      "\n",
      "                         95% CI Upper Bound  Odds Ratio  \n",
      "Nationality_Foreign                1.400326    3.308072  \n",
      "Nationality_G.C.C                 -0.983601    0.304321  \n",
      "Nationality_Not Stated            -1.237958    0.225150  \n",
      "Nationality_Other Arabs            0.644722    1.557097  \n",
      "Role_Passenger                     0.284022    1.123342  \n",
      "Role_Pedestrian                   -0.871489    0.352789  \n",
      "Gender_Male                        1.194223    2.873023  \n",
      "Year_Scaled                        0.160154    1.093377  \n",
      "Count_Scaled                      -7.392425    0.000575  \n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names after polynomial transformation\n",
    "feature_names = X_train.columns\n",
    "# Extract coefficients\n",
    "coefficients = pd.DataFrame(log_reg.coef_, columns=feature_names, index=[1])\n",
    "\n",
    "# Calculate standard errors of coefficients\n",
    "# Standard error = sqrt(diagonal of (X^T * X)^-1)\n",
    "X_poly_with_intercept = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "cov_matrix = np.linalg.inv(X_poly_with_intercept.T @ X_poly_with_intercept)\n",
    "standard_errors = np.sqrt(np.diag(cov_matrix))[1:]  # Exclude the intercept term standard error\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "z_score = norm.ppf(0.975)\n",
    "conf_intervals = pd.DataFrame({\n",
    "    'Coefficient': coefficients.values.flatten(),\n",
    "    'Standard Error': standard_errors,\n",
    "    '95% CI Lower Bound': coefficients.values.flatten() - z_score * standard_errors,\n",
    "    '95% CI Upper Bound': coefficients.values.flatten() + z_score * standard_errors,\n",
    "    'Odds Ratio': np.exp(coefficients.values.flatten())\n",
    "}, index=feature_names)\n",
    "\n",
    "# Print coefficients and confidence intervals\n",
    "print(\"Logistic Regression Coefficients and 95% Confidence Intervals:\")\n",
    "print(conf_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>95% CI Lower Bound</th>\n",
       "      <th>95% CI Upper Bound</th>\n",
       "      <th>Odds Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nationality_Foreign</th>\n",
       "      <td>1.196366</td>\n",
       "      <td>0.104064</td>\n",
       "      <td>0.992405</td>\n",
       "      <td>1.400326</td>\n",
       "      <td>3.308072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_Male</th>\n",
       "      <td>1.055365</td>\n",
       "      <td>0.070847</td>\n",
       "      <td>0.916507</td>\n",
       "      <td>1.194223</td>\n",
       "      <td>2.873023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nationality_Other Arabs</th>\n",
       "      <td>0.442823</td>\n",
       "      <td>0.103012</td>\n",
       "      <td>0.240924</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>1.557097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Role_Passenger</th>\n",
       "      <td>0.116309</td>\n",
       "      <td>0.085570</td>\n",
       "      <td>-0.051405</td>\n",
       "      <td>0.284022</td>\n",
       "      <td>1.123342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_Scaled</th>\n",
       "      <td>0.089271</td>\n",
       "      <td>0.036166</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.160154</td>\n",
       "      <td>1.093377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Role_Pedestrian</th>\n",
       "      <td>-1.041886</td>\n",
       "      <td>0.086939</td>\n",
       "      <td>-1.212282</td>\n",
       "      <td>-0.871489</td>\n",
       "      <td>0.352789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nationality_G.C.C</th>\n",
       "      <td>-1.189671</td>\n",
       "      <td>0.105140</td>\n",
       "      <td>-1.395741</td>\n",
       "      <td>-0.983601</td>\n",
       "      <td>0.304321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nationality_Not Stated</th>\n",
       "      <td>-1.490987</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>-1.744016</td>\n",
       "      <td>-1.237958</td>\n",
       "      <td>0.225150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count_Scaled</th>\n",
       "      <td>-7.461648</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>-7.530871</td>\n",
       "      <td>-7.392425</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Coefficient  Standard Error  95% CI Lower Bound  \\\n",
       "Nationality_Foreign         1.196366        0.104064            0.992405   \n",
       "Gender_Male                 1.055365        0.070847            0.916507   \n",
       "Nationality_Other Arabs     0.442823        0.103012            0.240924   \n",
       "Role_Passenger              0.116309        0.085570           -0.051405   \n",
       "Year_Scaled                 0.089271        0.036166            0.018387   \n",
       "Role_Pedestrian            -1.041886        0.086939           -1.212282   \n",
       "Nationality_G.C.C          -1.189671        0.105140           -1.395741   \n",
       "Nationality_Not Stated     -1.490987        0.129099           -1.744016   \n",
       "Count_Scaled               -7.461648        0.035319           -7.530871   \n",
       "\n",
       "                         95% CI Upper Bound  Odds Ratio  \n",
       "Nationality_Foreign                1.400326    3.308072  \n",
       "Gender_Male                        1.194223    2.873023  \n",
       "Nationality_Other Arabs            0.644722    1.557097  \n",
       "Role_Passenger                     0.284022    1.123342  \n",
       "Year_Scaled                        0.160154    1.093377  \n",
       "Role_Pedestrian                   -0.871489    0.352789  \n",
       "Nationality_G.C.C                 -0.983601    0.304321  \n",
       "Nationality_Not Stated            -1.237958    0.225150  \n",
       "Count_Scaled                      -7.392425    0.000575  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_intervals.sort_values(by=\"Odds Ratio\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_intervals.to_csv(\"./Odds Ratios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Feature Importance:\n",
      "Count_Scaled               0.650140\n",
      "Year_Scaled                0.084503\n",
      "Gender_Male                0.057467\n",
      "Nationality_G.C.C          0.053368\n",
      "Role_Pedestrian            0.043185\n",
      "Nationality_Not Stated     0.042206\n",
      "Nationality_Foreign        0.030404\n",
      "Nationality_Other Arabs    0.022610\n",
      "Role_Passenger             0.016117\n",
      "dtype: float64\n",
      "XGBoost Feature Importance:\n",
      "Count_Scaled               0.227289\n",
      "Gender_Male                0.159954\n",
      "Nationality_G.C.C          0.144220\n",
      "Nationality_Not Stated     0.126937\n",
      "Nationality_Foreign        0.126930\n",
      "Nationality_Other Arabs    0.086517\n",
      "Role_Pedestrian            0.081116\n",
      "Role_Passenger             0.037769\n",
      "Year_Scaled                0.009268\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Feature importance for Random Forest\n",
    "rf_importances = pd.Series(models['Random Forest'].feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_importances)\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "xgb_importances = pd.Series(models['XGBoost'].feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print(\"XGBoost Feature Importance:\")\n",
    "print(xgb_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
